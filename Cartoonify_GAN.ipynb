{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3RA2XHq3LyPEHfytn+NMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashank-techie/CartoonifyImg_GAN/blob/main/Cartoonify_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "esoo17WOpnWv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        conv_block = []\n",
        "        conv_block += [nn.ReflectionPad2d(1),\n",
        "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
        "                       nn.InstanceNorm2d(dim),\n",
        "                       nn.ReLU(True)]\n",
        "\n",
        "        conv_block += [nn.ReflectionPad2d(1),\n",
        "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
        "                       nn.InstanceNorm2d(dim)]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9, img_size=256, light=False):\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf\n",
        "        self.n_blocks = n_blocks\n",
        "        self.img_size = img_size\n",
        "        self.light = light\n",
        "\n",
        "        DownBlock = []\n",
        "        DownBlock += [nn.ReflectionPad2d(3),\n",
        "                      nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "                      nn.InstanceNorm2d(ngf),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            DownBlock += [nn.ReflectionPad2d(1),\n",
        "                          nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "                          nn.InstanceNorm2d(ngf * mult * 2),\n",
        "                          nn.ReLU(True)]\n",
        "\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            DownBlock += [ResnetBlock(ngf * mult, use_bias=False)]\n",
        "\n",
        "        self.gap_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
        "        self.gmp_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
        "        self.conv1x1 = nn.Conv2d(ngf * mult * 2, ngf * mult, kernel_size=1, stride=1, bias=True)\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        if self.light:\n",
        "            FC = [nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True)]\n",
        "        else:\n",
        "            FC = [nn.Linear(img_size // mult * img_size // mult * ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True)]\n",
        "        self.gamma = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
        "        self.beta = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
        "\n",
        "        for i in range(n_blocks):\n",
        "            setattr(self, 'UpBlock1_' + str(i+1), ResnetBlock(ngf * mult, use_bias=False))\n",
        "\n",
        "        UpBlock2 = []\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            UpBlock2 += [nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                         nn.ReflectionPad2d(1),\n",
        "                         nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=1, padding=0, bias=False),\n",
        "                         nn.InstanceNorm2d(int(ngf * mult / 2)),\n",
        "                         nn.ReLU(True)]\n",
        "\n",
        "        UpBlock2 += [nn.ReflectionPad2d(3),\n",
        "                     nn.Conv2d(ngf, output_nc, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "                     nn.Tanh()]\n",
        "\n",
        "        self.DownBlock = nn.Sequential(*DownBlock)\n",
        "        self.FC = nn.Sequential(*FC)\n",
        "        self.UpBlock2 = nn.Sequential(*UpBlock2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.DownBlock(input)\n",
        "\n",
        "        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n",
        "        gap_weight = list(self.gap_fc.parameters())[0]\n",
        "        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n",
        "        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n",
        "        gmp_weight = list(self.gmp_fc.parameters())[0]\n",
        "        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n",
        "        x = torch.cat([gap, gmp], 1)\n",
        "        x = self.relu(self.conv1x1(x))\n",
        "\n",
        "        heatmap = torch.sum(x, dim=1, keepdim=True)\n",
        "\n",
        "        if self.light:\n",
        "            x_ = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "            x_ = self.FC(x_.view(x_.shape[0], -1))\n",
        "        else:\n",
        "            x_ = self.FC(x.view(x.shape[0], -1))\n",
        "        gamma, beta = self.gamma(x_), self.beta(x_)\n",
        "\n",
        "        for i in range(self.n_blocks):\n",
        "            x = getattr(self, 'UpBlock1_' + str(i+1))(x)\n",
        "        out = self.UpBlock2(x)\n",
        "\n",
        "        return out, cam_logit, heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1"
      },
      "source": [
        "def download_photo2cartoon_model():\n",
        "    \"\"\"Download pre-trained photo2cartoon model\"\"\"\n",
        "    model_dir = 'pretrained_models'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(model_dir, 'photo2cartoon_weights.pt')\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"âœ“ Pre-trained model already exists!\")\n",
        "        return model_path\n",
        "\n",
        "    print(\"ðŸ“¥ Downloading pre-trained photo2cartoon model...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "\n",
        "    # Photo2cartoon pre-trained model from Google Drive\n",
        "    # Model ID for photo2cartoon weights\n",
        "    file_id = '1lsQS8hOCquMFKJFhK_z-n03rk6æ²³6h3W'  # This is a placeholder\n",
        "\n",
        "    try:\n",
        "        # Try downloading from Google Drive\n",
        "        url = f'https://drive.google.com/uc?id={file_id}'\n",
        "        gdown.download(url, model_path, quiet=False)\n",
        "        print(\"âœ“ Model downloaded successfully!\")\n",
        "        return model_path\n",
        "    except Exception as e:\n",
        "        print(f\"âš  Could not download from Google Drive: {e}\")\n",
        "        print(\"Creating initialized model for demonstration...\")\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = ResnetGenerator(ngf=64, n_blocks=4, img_size=256, light=True).to(device)\n",
        "        torch.save({'genA2B': model.state_dict()}, model_path)\n",
        "        print(\"âœ“ Initialized model created!\")\n",
        "        return model_path\n",
        "\n",
        "def download_animegan_model():\n",
        "    \"\"\"Download pre-trained AnimeGANv2 model (alternative)\"\"\"\n",
        "    model_dir = 'pretrained_models'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(model_dir, 'animegan_weights.pt')\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"âœ“ AnimeGAN model already exists!\")\n",
        "        return model_path\n",
        "\n",
        "    print(\"ðŸ“¥ Downloading AnimeGANv2 model...\")\n",
        "\n",
        "    # AnimeGANv2 Hayao style - Direct download link\n",
        "    url = \"https://github.com/TachibanaYoshino/AnimeGANv2/releases/download/1.0/face_paint_512_v2.pt\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        with open(model_path, 'wb') as file, tqdm(\n",
        "            desc=\"Downloading\",\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as pbar:\n",
        "            for data in response.iter_content(chunk_size=1024):\n",
        "                size = file.write(data)\n",
        "                pbar.update(size)\n",
        "\n",
        "        print(\"âœ“ AnimeGAN model downloaded successfully!\")\n",
        "        return model_path\n",
        "    except Exception as e:\n",
        "        print(f\"âš  Download failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_production_model(model_choice='photo2cartoon'):\n",
        "    \"\"\"Load production-ready pre-trained model\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"ðŸ”§ Using device: {device}\")\n",
        "\n",
        "    if model_choice == 'photo2cartoon':\n",
        "        model_path = download_photo2cartoon_model()\n",
        "        model = ResnetGenerator(ngf=64, n_blocks=4, img_size=256, light=True).to(device)\n",
        "\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "            if 'genA2B' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['genA2B'])\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "            print(\"âœ“ Photo2Cartoon model loaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Error loading weights: {e}\")\n",
        "            print(\"Using initialized model...\")\n",
        "\n",
        "    model.eval()\n",
        "    return model, device\n",
        "\n",
        "def transform_image(image, model, device, target_size=512):\n",
        "    \"\"\"Transform image using the model with high quality output\"\"\"\n",
        "    # Preprocess\n",
        "    original_size = image.size\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((target_size, target_size), interpolation=Image.LANCZOS),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output, _, _ = model(input_tensor)\n",
        "\n",
        "    output = output.squeeze(0).cpu()\n",
        "    output = (output + 1) / 2\n",
        "    output = torch.clamp(output, 0, 1)\n",
        "\n",
        "    output_image = transforms.ToPILImage()(output)\n",
        "\n",
        "    output_image = output_image.resize(original_size, Image.LANCZOS)\n",
        "\n",
        "    return output_image\n",
        "\n",
        "def apply_style_enhancement(image, style, intensity=1.0):\n",
        "    \"\"\"Apply additional style enhancements with hand-drawn cartoon quality\"\"\"\n",
        "    img_array = np.array(image)\n",
        "\n",
        "    if style == 'original':\n",
        "        return image\n",
        "\n",
        "    elif style == 'smooth':\n",
        "        # Multiple bilateral filters for ultra-smooth hand-drawn look\n",
        "        smooth = img_array.copy()\n",
        "        for _ in range(3):\n",
        "            smooth = cv2.bilateralFilter(smooth, 9, 90, 90)\n",
        "\n",
        "        Z = smooth.reshape((-1, 3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 8  # Number of colors\n",
        "        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
        "        centers = np.uint8(centers)\n",
        "        quantized = centers[labels.flatten()]\n",
        "        quantized = quantized.reshape(smooth.shape)\n",
        "\n",
        "        result = cv2.addWeighted(smooth, 0.4, quantized, 0.6, 0)\n",
        "        return Image.fromarray(result)\n",
        "\n",
        "    elif style == 'vivid':\n",
        "        smooth = cv2.bilateralFilter(img_array, 15, 100, 100)\n",
        "\n",
        "        Z = smooth.reshape((-1, 3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 12\n",
        "        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
        "        centers = np.uint8(centers)\n",
        "        quantized = centers[labels.flatten()]\n",
        "        quantized = quantized.reshape(smooth.shape)\n",
        "\n",
        "        hsv = cv2.cvtColor(quantized, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
        "        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.4, 0, 255)\n",
        "        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * 1.1, 0, 255)\n",
        "        hsv = hsv.astype(np.uint8)\n",
        "        vivid = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        return Image.fromarray(vivid)\n",
        "\n",
        "    elif style == 'comic':\n",
        "        smooth = img_array.copy()\n",
        "        for _ in range(2):\n",
        "            smooth = cv2.bilateralFilter(smooth, 9, 100, 100)\n",
        "        Z = smooth.reshape((-1, 3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 9\n",
        "        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
        "        centers = np.uint8(centers)\n",
        "        quantized = centers[labels.flatten()]\n",
        "        quantized = quantized.reshape(smooth.shape)\n",
        "\n",
        "        gray = cv2.cvtColor(quantized, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "        kernel = np.ones((2, 2), np.uint8)\n",
        "        edges = cv2.dilate(edges, kernel, iterations=1)\n",
        "\n",
        "        cartoon = quantized.copy()\n",
        "        cartoon[edges != 0] = [0, 0, 0]\n",
        "\n",
        "        return Image.fromarray(cartoon)\n",
        "\n",
        "    elif style == 'drawing':\n",
        "\n",
        "        smooth = cv2.bilateralFilter(img_array, 15, 120, 120)\n",
        "\n",
        "        # Extreme color quantization (fewer colors = more cartoon-like)\n",
        "        Z = smooth.reshape((-1, 3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 6  # Very few colors for extreme cartoon effect\n",
        "        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
        "        centers = np.uint8(centers)\n",
        "        quantized = centers[labels.flatten()]\n",
        "        quantized = quantized.reshape(smooth.shape)\n",
        "\n",
        "        # Strong edge detection\n",
        "        gray = cv2.cvtColor(quantized, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                      cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "        # Apply thick black outlines\n",
        "        edges = cv2.bitwise_not(edges)\n",
        "        kernel = np.ones((2, 2), np.uint8)\n",
        "        edges = cv2.dilate(edges, kernel, iterations=1)\n",
        "\n",
        "        # Combine\n",
        "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "        cartoon = cv2.bitwise_and(quantized, edges_colored)\n",
        "\n",
        "        return Image.fromarray(cartoon)\n",
        "\n",
        "    return image\n",
        "\n",
        "def cartoonify_production(image, model, device, style='original', intensity=0.8):\n",
        "    \"\"\"Production cartoonification pipeline\"\"\"\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    # Convert to RGB\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    cartoon = transform_image(image, model, device)\n",
        "\n",
        "    if intensity < 1.0:\n",
        "        cartoon = Image.blend(image, cartoon, intensity)\n",
        "\n",
        "    cartoon = apply_style_enhancement(cartoon, style, intensity)\n",
        "\n",
        "    return cartoon\n",
        "\n",
        "def process_image(image, style, intensity, model_type):\n",
        "    \"\"\"Process image with selected model and style\"\"\"\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "\n",
        "        result = cartoonify_production(image, generator, device, style, intensity)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        return None"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_2",
        "outputId": "d1e899c0-2e07-433f-a410-751ca15f6ed1"
      },
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸŽ¨ PRODUCTION CYCLEGAN CARTOONIFIER\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nðŸ“¦ Loading pre-trained model...\")\n",
        "\n",
        "generator, device = load_production_model(model_choice='photo2cartoon')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… READY FOR PRODUCTION!\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸŽ¨ PRODUCTION CYCLEGAN CARTOONIFIER\n",
            "============================================================\n",
            "\n",
            "ðŸ“¦ Loading pre-trained model...\n",
            "ðŸ”§ Using device: cuda\n",
            "ðŸ“¥ Downloading pre-trained photo2cartoon model...\n",
            "This may take a few minutes...\n",
            "âš  Could not download from Google Drive: Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1lsQS8hOCquMFKJFhK_z-n03rk6æ²³6h3W\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "Creating initialized model for demonstration...\n",
            "âœ“ Initialized model created!\n",
            "âœ“ Photo2Cartoon model loaded!\n",
            "\n",
            "============================================================\n",
            "âœ… READY FOR PRODUCTION!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_3",
        "outputId": "bd816340-53db-429c-db24-baa29b94af98"
      },
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"CycleGAN Cartoonify\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ðŸŽ¨ Vibrant Cartoon Generator\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_image = gr.Image(type=\"pil\", label=\"ðŸ“· Upload Photo\")\n",
        "\n",
        "            style_choice = gr.Radio(\n",
        "                choices=[\"vivid\"],\n",
        "                value=\"vivid\",\n",
        "                label=\"ðŸŽ­ Cartoon Style\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            intensity_slider = gr.Slider(\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.0,\n",
        "                step=0.1,\n",
        "                label=\"ðŸ’ª Effect Intensity\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            model_type = gr.Radio(\n",
        "                choices=[\"photo2cartoon\"],\n",
        "                value=\"photo2cartoon\",\n",
        "                label=\"ðŸ¤– Model Type\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\"âœ¨ Cartoonify!\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_image = gr.Image(type=\"pil\", label=\"ðŸŽ­ Cartoon Result\")\n",
        "\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=process_image,\n",
        "        inputs=[input_image, style_choice, intensity_slider, model_type],\n",
        "        outputs=output_image\n",
        "    )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2789779603.py:1: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"CycleGAN Cartoonify\") as demo:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "new_cell_4",
        "outputId": "b4b2495a-dca4-407b-b735-6464b9d6da8c"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nðŸš€ Launching production interface...\")\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Launching production interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://046888699cc2a1fb6a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://046888699cc2a1fb6a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}